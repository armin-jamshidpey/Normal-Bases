\section{Computing projections of the orbit sum}\label{sec:osum}

So far we have reduced the problem of invertiblity of $M_G(\alpha)$ to $l(\osum{G}{K}) \in F[G]$ for a generic
projection $l$. In order to compute $l(\osum{G}{K})$ we need $l(g(\alpha)) \in F$ for $g \in G$. In this section we present 
algorithms to compute the mentioned terms which lead to compute $\osum{G}{K}$ in case that $G$ is either abelian or metacyclic. 
The best way to start is to take a look at the simplest case which is the cyclic one. 

Suppose $G = \langle g \rangle$
where $g$ is the frobenius map. In \cite{Kaltofen} the problem of
computing 
\begin{equation}\label{eq:cycproj}
l(g^i(\alpha)), 0 \leq i \leq n-1,
\end{equation}
is called automorphism projection problem and an algorithm to solve 
the problem is introduced. Here we briefly explain the idea of the mentioned algorithm and later on we introduce a
generalized version of it which works for any finite abelian group.

With $l,g,G$ as above, $\alpha \in K$ and $Q$ as the corresponding matrix to Frobenius map, the values in \eqref{eq:cycproj} is rewritten as
$$(lQ^{tj})\cdot(Q^i\alpha), \,\, 0 \leq j <m, 0 \leq i <t$$
 for a chosen value $t$, can be computed in three steps. The first step is to compute $Q^i\alpha$ for $0 \leq i <t$. Afterwards
 compute $lQ^{tj}$ for $0 \leq j <m$ by doing $m-1$ transposed modular composition which in turn uses an algorithm given
 in \cite{Shoup}. At the end the terms in \eqref{eq:cycproj} can be computed by a matrix multiplication.
 
the details of above computations in \cite{Kaltofen} is presented useing 3 lemmas. At this point we provide the following 
lemmas which are variants of \cite[Lemma 3, Lemma 4 $\&$ Lemma 8]{Kaltofen}, with a slight modification in the proofs.

\begin{lemma}\cite{Kaltofen}\label{lem:modcom}
Assume $K$ is a field, $f\in K[x]$ is of degree $n$ and $s = \lceil\sqrt{n}\rceil$. Given $g_1, \ldots , g_{s}$ and 
$h \in K[x]$ of degree less than $n$, $$g_1(h), \ldots g_{s}(h) \mod f$$ can be computed in
$O(n^{\frac{3}{4}\omega(\frac{4}{3})})$ where $\omega(\frac{4}{3})$ is the exponent of rectangular matrix 
multiplication as introduced in \cite{LeGall}. 
\end{lemma}

\begin{proof}
Let $t = \lceil n^{3/4} \rceil$ and rewrite $g_1 , \ldots , g_s$ as 
$$g_i = \sum_{0 \leq j < n/t} g_{ij}x^{tj}.$$
Now $g_{ij}$'s are polynomials of degree less than $t$. The next step is to compute $H_i = h^i \mod f$ for $i = 0 , \ldots , t$.
Having $H_i$'s in hand one can form the matrix $H = \left[ H_1 \vert \cdots \vert H_t \right]^T$ where each column is the vector of 
the element $H_i$ (with coefficients in $K$) so the matrix $H$ is of size $t \times n$. We form 
$$A = \left[\bar{g}_{10}\vert \cdots \vert \bar{g}_{1(n/t-1)}\vert \cdots \vert \bar{g}_{s0}\vert \cdots \vert \bar{g}_{s(n/t-1)}\right]^T,$$
where $\bar{g}_{ij}$ is the  vector of $g_{ij}$. In order to compute $g_{ij}(h)$ one can compute $A \cdot H$. Using 
results from \cite{LeGall}, this can be done in $O(n^{3/4 \omega(4/3)})$. The last step to get $g_i(h)$, is to substitute $H_t$ 
for $x^t$ and performing a Horner evaluation scheme. The dominant term in the cost of this calculation, is the cost of computing $AH$ which can be carried out using $O(n^{3/4 \omega(4/3)})$ operations in $K$.
\end{proof}

\begin{lemma}\cite{Kaltofen}\label{lem:selfcomp}
Assume $K = F[x]/f$ is a finite Galois extension of $F$, for an irreducible $f\in F[x]$ of degree $n $. Given 
$\lbrace s_1, \ldots s_r \rbrace \subset \mathbb{N}$
such that $\prod_{i = 1}^r s_i = \lceil\sqrt{n}\rceil$, $g_1, \ldots , g_{r} \in G = \mathrm{Gal}(K/F)$ with their actions on 
$\bar{x}=x \mod f$ and $\alpha \in K$,
$$g_1^{i_1}\cdots g_r^{i_r}(\alpha) , 1 \leq j \leq r, 
0 \leq i_j \leq s_i$$ can be 
computed in $\osumcosttilde$ where $\omega(\frac{4}{3})$ is the exponent of rectangular matrix 
multiplication as introduced in \cite{LeGall}. 
\end{lemma}

\begin{proof}
Suppose $\beta = \sum_{0\leq i \leq n-1} a_i \bar{x}^i \in F[\bar{x}]$. For any $g_j$ we have 
\begin{equation}\label{eq:comute}
g_j^t(\beta) = \sum_{0\leq i \leq n-1} a_i g_j^t(\bar{x}^i) = \beta(g_j^t(\bar{x}))
\end{equation}
since $g_j$ acts trivially on $a_i$. 

Assume we have computed $$g_1^{i_1}\cdots g_r^{i_r}(\alpha) , 1 \leq j \leq r, 
0 \leq i_j \leq k_j, \prod_{j = 1}^r k_j \leq \lceil \sqrt{n} \rceil/2$$
Then we can get $$g_1^{i_1}\cdots g_r^{i_r}(\alpha) , j \in \lbrace 1, \ldots , r \rbrace \setminus \lbrace t \rbrace , 
0 \leq i_j \leq k_j, 0 \leq i_t \leq 2k_t,$$
by computing 
$$g_t^{k_t}(g_1^{i_1}\cdots g_r^{i_r}(\alpha)) , 1 \leq j \leq r, 
0 \leq i_j \leq k_j,$$
which can be carried out using $\osumcost$ operations in $F$ by applying Lemma \ref{lem:modcom} and using \eqref{eq:comute}.
Using above doubling method for $g_i$, we have to do $O(\log s_i)$ iterations with $\osumcost$ operations in $F$. Hence the total cost of this 
computation is $\osumcosttilde$

%
%
%Now assume we have computed 
% $$g_1(\alpha), \ldots , g_1^m(\alpha).$$ Then we can compute  $g_1^{m+1}(\alpha), \ldots , g_1^{2m}(\alpha)$ by computing $$g_1^m(g_1(\alpha)), \ldots , g_1^m(g_1^m(\alpha)) $$ using Lemma \ref{modcom} and Equation \ref{eq:comute}. By applying the above 
% doubling method, the computation of $g_1(\alpha), \ldots , g_1^{s_1}(\alpha)$ can be done in $ O(\log s_1) \times \osumcost$.
% Note that here we need to compute the action of (even) powers of $g_1$ on $\bar{x}$ as well. However, it is just a one time
% computation and does not change the complexity. 
%
%The next step is to apply $g_2^i$ to $g_1(\alpha), \ldots , g_1^{s_1}(\alpha)$ for $0\leq i \leq s_2$. Again 
%$g_1^j(\alpha) = \sum_{i} a_{ij} \bar{x}^i$ and $$g_2^t(g_1^j(\alpha)) = \sum_{i} a_{ij} (g_2^t(\bar{x}))^i = g_1^j(\alpha)(g_2^t(\bar{x})).$$
%Using the above fact we compute 
%$$g_2(g_1(\alpha)), \ldots , g_2(g_1^{s_1}(\alpha))$$
%by applying Lemma \ref{modcom} which costs $\osumcost$. At this point we have 
%$$\begin{array}{lll} g_1(\alpha)& \ldots & g_1^{s_1}(\alpha)\\ g_2(g_1(\alpha))& \ldots & g_2(g_1^{s_1}(\alpha))\end{array}$$
%and by applying $g_2^2$ (using the above method) we get 
%$$\begin{array}{lll} g_1(\alpha)& \ldots & g_1^{s_1}(\alpha)\\ g_2(g_1(\alpha))& \ldots & g_2(g_1^{s_1}(\alpha))\\
%g_2^2(g_1(\alpha))& \ldots & g_2^2(g_1^{s_1}(\alpha))\\g_2^3(g_1(\alpha))& \ldots & g_2^3(g_1^{s_1}(\alpha))
%\end{array}$$
%then by repeating the above doubling method we get
%$$g_2^{i_2}(g_1^{i_1}(\alpha)), \, 0\leq i_1\leq s_1, 0 \leq i_2 \leq s_2,$$
%using $O(\log s_2)\cdot \osumcost + O(\log s_1)\cdot \osumcost$ operations in $F$ (in total), which is equal to $O(\log (s_1s_2))\osumcost$. 
%
%Doing the same for $g_3 , \ldots , g_r$ we see the cost of the computation is 
%$$O(\log (s_1\cdots s_r))\osumcost = \osumcosttilde.$$

Note that for each $g_i$ we have to compute $g_i^j$ for $0 \leq j \leq s_i$ but this is just a one time computation for each $g_i$
which does not change the total cost.
\end{proof}

\begin{lemma}\label{lem:transmodcomp}
Assume $K = F[x]/f = F[\bar{x}]$ is a finite Galois extension of $F$ where $f$ is irreducible of degree $n$. Moreover,
let $ \lbrace g_i \rbrace_{1\leq i \leq r} \subset \mathrm{Gal}(K/F) $, $\lbrace s_1, \ldots, s_r \rbrace \subset \mathbb{N}$
such that $\prod_{i = 1}^r s_i \leq \lceil \sqrt{n_i} \rceil$
 and $L: K\rightarrow F$ is a linear projection. The linear maps
$$L \circ g_1^{i_1} \cdots g_r^{i_r}, \,\, 1\leq j \leq r, 0 \leq i_j < s_j, $$ can be computed using $\osumcosttilde$ operations in $F$. 
\end{lemma} 

\begin{proof}
Let $T_{i_1\cdots i_r} = L \circ g_1^{i_1}\cdots g_r^{i_r}$. Here we use a doubling method. Assume we have have access to 
$$T_{i_1\cdots i_r}, \, \mathrm{for}\,\, 1\leq j \leq r, 0 \leq i_j \leq k_j.$$
Then we can compute 
$$T_{i_1\cdots i_r}, \, \mathrm{for}\,\, j \in \lbrace 1, \ldots, r\rbrace \setminus \lbrace t \rbrace, 0 \leq i_j \leq k_j, 0 \leq i_t \leq 2k_t$$
in the following way. We compute 
$T_{i_1 \cdots i_r} \circ g_t^{k_t}, \,\, 1\leq j \leq r, 0 \leq i_j \leq k_j.$
, where $0\leq j \leq k_t$ is the $t$-th index. Since a linear projection is determined by its action on a basis, such as 
$\lbrace\bar{x}^i: 0\leq i \leq n-1 \rbrace$, the corresponding row vector to $T_{i_1 \cdots i_r} \circ g_t^{k_t}$ is 
\begin{equation}\label{eq:projvec}
\begin{bmatrix} T_{i_1 \cdots i_r}(u_{t,k_t}^0) & \cdots & T_{i_1 \cdots i_r}(u_{t,k_t}^{n-1}) \end{bmatrix}
\end{equation}
where $u_{t,k_t} = g_t^{k_t}(\bar{x})$. In order to compute the above row vector for a fixed index $i_1 \cdots i_r$, let $r = \lceil n^{1/4} \rceil$, compute
$$\overline{T_{i_1 \cdots i_r}} = \left[\begin{array}{c|c|c}
u_{t,k_t}^0\cdot T_{i_1 \cdots i_r} & \cdots & u_{t,k_t}^{r-1}\cdot T_{i_1 \cdots i_r}
\end{array} \right]$$
and multiply it by 
$$ U = \left[
\begin{array}{c}
u_{t,k_t}^0\\
\hline
u_{t,k_t}^{r}\\
\hline
u_{t,k_t}^{2r}\\
\hline
\vdots\\
\hline
u_{t,k_t}^{(\lceil n^{3/4} \rceil-1)r}
\end{array} \right].$$
Note that the term $u_{t,k_t}^i \cdot T_{i_1 \cdots j \cdots i_r}$, is the so called transposed modular multiplication and one can apply the algorithm introduced in \cite{Shoup} to compute it. It is clear that $U$ is of size 
$\lceil n^{3/4} \rceil \times n$ and $\overline{T_{i_1 \cdots i_r}}$ is an $n \times n^{1/4}$. Finally the vectors in \eqref{eq:projvec}
is given by
$$U \cdot \left[\begin{array}{c|c|c}
\overline{T_{i_1 \cdots i_{t-1},1, i_{t+1} \cdots i_r}} & \cdots & \overline{T_{i_1 \cdots i_{t-1},k_t,i_{t+1} \cdots i_r}}
\end{array}\right].
$$
This completes the doubling method.

Note that in the last step of above doubling method for a fixed index $t$ we have $k_t = s_t/2$. Hence for $g_t$ the above computation can be done 
using $O(\log s_t)$ times the cost of the matrix multiplication which is $\osumcost$. Since we should repeat the above process for each $g_t$ the total cost is
$\osumcosttilde$ 
\end{proof}

%Now we have enough tools for computing the automorphism projection in the cyclic case where $G = \langle g: g^n =1 \rangle$
%Which is basically \cite[Algorithm AP]{Kaltofen}. 
%Assume $L = \begin{bmatrix}
%l_1 & l_2 & \ldots & l_n
%\end{bmatrix}$ is given where $l_i \in F$. The goal is to compute 
%\begin{equation}\label{eq:autproj}
%L(g^i(\alpha)) = (L \circ g^{j \lceil \sqrt{n} \rceil})\cdot (g^i(\alpha)), 0 \leq j < \lceil \sqrt{n} \rceil, 0 \leq i \leq \lceil \sqrt{n} \rceil -1, 
%\end{equation}
%for a given $\alpha \in K$. 
%%For a given polynomials $h,f \in F[x]$ and projection $L$, computing $L(h^i \mod f)$ for
%%$0\leq i \leq r$ is called transposed modular composition. In order to do such computation in \cite{Shoup} has defined a subtask which is called transposed modular multiplication and 
%Let $T_j = L\circ g^{j \lceil \sqrt{n} \rceil}$.
%%and this operation is called  note that in \cite{Shoup} . 
%This computation can be done in 3 steps:
%
%\textbf{step 1.} compute $g^i(\alpha)$ for $i = 1, \ldots , \lceil \sqrt{n} \rceil$. Using Lemma \ref{lem:selfcomp} this 
%can be carried out by doing $\osumcosttilde$ operations in $F$.
%
%\textbf{step 2.} apply the algorithm in Lemma \ref{lem:transmodcomp} with $g^ {\lceil \sqrt{n} \rceil} \in G$ to get the linear projections 
%$$L \circ g^{j \lceil \sqrt{n} \rceil}, \,\, 0 \leq j < \lceil \sqrt{n} \rceil.$$ 

%\textbf{step 2.} If we have $T_1, \ldots , T_m$, then we can get $T_{m+1}, \ldots T_{2m}$ by computing $T_i \circ g^{m\sqrt{n}}$
%which is equivalent of computing 
%\begin{equation}\label{eq:projdoubling}
%\begin{bmatrix}  T_i(s^0) & \cdots & T_i(s^{n-1}) \end{bmatrix}
%\end{equation}
%where $s_m = g^{m\sqrt{n}}(\bar{x})$. In order to compute row vector presented in \ref{eq:projdoubling} for a fixed $i$, let 
%$r = \lceil n^{1/4} \rceil$ and compute 
%$$
%\overline{T_i} = \left[\begin{array}{c|c|c}
%s_m^0\cdot T_1 & \cdots & s_m^{r-1}\cdot T_i
%\end{array} \right]
%$$
%and multiply it by 
%$$S = \left[
%\begin{array}{c}
%s_m^0\\
%\hline
%s_m^{r}\\
%\hline
%s_m^{2r}\\
%\hline
%\vdots\\
%\hline
%s_m^{(n^{3/4}-1)r}
%\end{array} \right].
%$$
%Note that in the above computation one can use the algorithm given in \cite{Shoup}
%for transposed modular composition. 
%One can see $\tilde{T_i} = S \cdot \overline{T_i}$ and $T_{m+1}, \ldots, T_{2m}$ can be computed by doing
%$$ S \cdot \begin{bmatrix} \overline{T_1}\vline & \cdots \vline & \overline{T_m} \end{bmatrix}$$
%where the second matrix is of size $n \times m$. This completes the doubling method. In the last step of the doubling method
%$m = \sqrt{n}/2$ so the matrix product can be computed using $\osumcost$ operations in $F$ which gives the cost $\osumcosttilde$
%for this step.

%\textbf{step 2.} compute $n/\lceil \sqrt{n} \rceil$ elements $h_i = \sum_{j = 1}^ {\lceil \sqrt{n} \rceil-1}c_{ij}g^{j}(\alpha)$ which is actually a rectangular matrix multiplication 
%\begin{equation}
%\left(\begin{array}{llll}
%c_{00} & c_{01} & \cdots & c_{0\lceil \sqrt{n} \rceil -1}\\
%c_{10} & c_{11} & \cdots & c_{1\lceil \sqrt{n} \rceil -1}\\
%\vdots & \vdots & \vdots & \vdots \\
%c_{00} & c_{01} & \cdots & c_{0\lceil \sqrt{n} \rceil -1}\\
%\end{array}\right)\cdot 
%\left(\begin{array}{l}
%\vv{g^0(\alpha)}\\
%\vv{g^1(\alpha)}\\
%\vdots\\
%\vv{g^{\lceil \sqrt{n} \rceil -1}(\alpha)}
%\end{array}\right),
%\end{equation}
%where $\vv{g^i(\alpha)} \in F^n$ is the vector of coefficients of $g^i(\alpha)$ in $F$. The multiplication can be 
%carried out using $\osumcost$ operations in $F$.

%\textbf{step 3.} So far we have computed $T_j$'s and $g^i(\alpha)$'s. The result can be acheived by doing the following 
%rectangular matrix multiplication:
%
%$$
%\begin{bmatrix}
%T_0 & \vert & \cdots & \vert &T_{\lceil \sqrt{n} \rceil -1}
%\end{bmatrix}^T \cdot
%\begin{bmatrix}
%\vv{g^0(\alpha)} & \vert & \cdots & \vert & \vv{g^{\lceil \sqrt{n} \rceil -1}}
%\end{bmatrix}.
%$$
%Since the left matrix is of size $\lceil \sqrt{n} \rceil \times n$ and the right matrix is of size $n \times \lceil \sqrt{n} \rceil$,
%using results from \cite{LeGall} this can be done by performing $O(n^{1/2\omega(2)})$ operations in $F$.
%
%One can verify that the dominant term in the cost of computations is the cost of step 2 which uses $\osumcosttilde$.

%\textbf{step 3.} Finally we want $\sum_{i = 0}^{n/\lceil \sqrt{n} \rceil} g^{i\lceil \sqrt{n} \rceil}(h_i)$. Let
%$\bar{g} = g^{\lceil \sqrt{n} \rceil}$ which is computed in step 1 already. We apply powers of $\bar{g}$ $h_i$'s in
%the following way: 
%\begin{footnotesize}
%$$
%\begin{array}{lllllllll}
%&h_0 & h_1 & h_2 & h_3 & h_4 & h_5 & h_6 &  \cdots \\
%\bar{g} \rightarrow &	& \downarrow & & \downarrow & & \downarrow & &  \cdots \\
%&h_0 & g(h_1) & h_2 & g(h_3) & h_4 & g(h_5) & h_6 &  \cdots \\	
%\bar{g}^2 \rightarrow &	&  & \downarrow& \downarrow & &  &\downarrow &  \cdots \\
%&h_0 & g(h_1) & g^2(h_2) & g^3(h_3) & h_4 & g(h_5) & g^2(h_6) & \cdots \\	
%\bar{g}^4 \rightarrow&	&  & &  & \downarrow& \downarrow &\downarrow & \cdots \\
%&h_0 & g(h_1) & g^2(h_2) & g^3(h_3) & g^4(h_4) & g^5(h_5) & g^6(h_6) &  \cdots \\		
%\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots 
%\end{array}
%$$
%\end{footnotesize}
%This means we do $O(\log \sqrt{n})$ times $\osumcost$ or equivalently $\osumcosttilde$ operations in $F$. 

At this point we have enough tools to see how the computation is done in the cyclic case. Moreover, we can use the above lemmas
to give an algorithm for a more general case, namely the abelian case. With a little bit more work we can state an algorithm 
which solves the automorphism projection problem when $G = \lbrace a^ib^j \rbrace$ where $m \leq n$. As an specific case this
solves the problem for metacyclic groups.

\subsection{Abelian Groups}
Assume $G$ is an abelian group presented as 
$$ \langle g_1, \ldots , g_r: g_{1}^{e_1} = \cdots = g_{r}^{e_r} = 1 \rangle$$
 where $ e_i \in \mathbb{N}$
is the order of $g_i$ and $n = e_1 \cdots e_r$. Moreover, let $s_i = \lceil	\sqrt{e_i \rceil}$ for $ 1\leq i \leq r$.
 Our goal is to compute 
\begin{equation}\label{eq:abelian}
L (g_1^{i_1},  \ldots, g_r^{i_r}(\alpha)), \, 1 \leq j \leq r, 0 \leq i_j \leq e_j
\end{equation}
 where $L = \begin{bmatrix} l_1 & \cdots l_n \end{bmatrix}$ is a given projection from $K$ to $F$. 
similar to the cyclic case, the elements in \eqref{eq:abelian} can be presented as 
\begin{equation}
\begin{split}
(L \circ g_1^{s_1j_1} \cdots g_r^{s_rj_s})\cdot (g_1^{i_1} \cdots g_r^{i_r}(\alpha))\\ 1\leq m \leq r, 0\leq i_m < s_m, 0 \leq j_m < s_m
\end{split}
\end{equation}
note that $T_{j_1\cdots j_r} = L \circ g_1^{s_1j_1} \cdots g_r^{s_rj_s}$ are linear projections 
presented as row vectors and $g_1^{i_1} \cdots g_r^{i_r}(\alpha)$ are field elements presented as column vectors. All elements in \eqref{eq:abelian} can be computed in 3 steps.

\textbf{Step 1.} Apply Lemma \ref{lem:selfcomp} to get 
$$g_1^{i_1} \cdots g_r^{i_r}(\alpha), \,\, 1\leq m \leq r, 0\leq i_m < s_m,$$
with cost $\osumcosttilde$

\textbf{Step 2.} Use Lemma \ref{lem:transmodcomp} to compute 
$$T_{j_1\cdots j_r}, \,\, 1\leq m \leq r, 0 \leq j_m < s_m,$$
with cost $\osumtilde$

\textbf{Step 3.} The last step is to the following matrix multiplication

$$
\left[ \begin{array}{c}
T_{00\cdots 0}\\
\hline
T_{10\cdots 0}\\
\hline
\vdots\\
\hline
T_{s_1 \cdots s_r}
\end{array} \right]
\cdot
\left[\begin{array}{c}
g_1^{0}\cdots g_r^{0}(\alpha) \\
\hline
g_1^{1}\cdots g_r^{0}(\alpha) \\
\hline
\cdots \\
\hline
g_1^{s_1}\cdots g_r^{s_r}(\alpha)  
\end{array}\right]^t.
$$
Using results for rectangular matrix multiplication we can compute the above product in $O(n^{1/2\omega(2)})$ operations in $F$.
%
%$$\sum_{\substack{0 \leq i_1 < e_1/t_1 \\ \vdots \\ 0 \leq i_n < e_n/t_n}} \hat{g}_1^{i_1} \cdots \hat{g}_n^{i_n}(\sum_{\substack{0 \leq j_1 \leq t_1-1\\ \vdots \\ 0 \leq j_n \leq t_n-1}} a^{i_1, \ldots , i_n}_{j_1, \ldots , j_n}g_1^{j_1}\cdots g_n^{j_n}(\alpha))$$
%where $t_i = \lceil \sqrt{e_i}\rceil$ and $\hat{g_i} = g_i^{t_i}.$
%
%It is simpler to think of monomials in $g_i$'s as integral points of $\mathbb{R}^n$ space, i.e.
%$(a_1, \ldots , a_n)$ represents $g_1^{a_1}, \ldots g_n^{a_n}$. If this is the case then the above equation suggests to 
%partition the $n$-cell $$[0,e_1] \times \cdots \times [0, e_n]$$ into $n$-cells of the same size as 
%$$[0,\lceil \sqrt{e_1} \rceil] \times \cdots \times [0, \lceil \sqrt{e_n} \rceil],$$ 
%then we can start to compute the automorphism evaluations in the $n$-cell containing the origin. After that each time we apply one 
%of the generators we are moving to another $n$-cell.
%
%We compute the projection of the orbit sum of $\alpha$ in 3 steps.
%
%\textbf{step 1. Compute the actions of the points in the $n$-cell containing the origin on $\alpha$}. At first we compute 
%$g_1(\alpha), \ldots , g_1^{t_1}(\alpha)$ in $\tilde{O}(\sqrt{\vert G \vert}^{\omega({\frac{3}{4}})})$ operations in $F$, by lemma 4. Now
%we apply 
%
% $g_2$ to $g_1(\alpha), \ldots , g_1^{t_1}(\alpha)$ then after applying $g_2^2$ to
%\[
% \begin{array}{lll}
% g_1(\alpha)& \ldots & g_1^{t_1}(\alpha)\\
% g_2(g_1(\alpha))& \ldots & g_2(g_1^{t_1}(\alpha)) 
%\end{array} 
%\]
%we have computed
%\[
% \begin{array}{lll}
% g_1(\alpha)& \ldots & g_1^{t_1}(\alpha)\\
% g_2(g_1(\alpha))& \ldots & g_2(g_1^{t_1}(\alpha)) \\
% g_2^2(g_1(\alpha))& \ldots & g_2^2(g_1^{t_1}(\alpha)) \\
% g_2^3(g_1(\alpha))& \ldots & g_2^3(g_1^{t_1}(\alpha))
%\end{array} 
%\]
%If we continue by applying $g_2^4, \ldots$ we are able to get
%\[
% \begin{array}{lll}
% g_1(\alpha)& \ldots & g_1^{t_1}(\alpha)\\
% g_2(g_1(\alpha))& \ldots & g_2(g_1^{t_1}(\alpha)) \\
% \vdots & \vdots & \vdots\\
% g_2^{t_2}(g_1(\alpha))& \ldots & g_2^{t_2}(g_1^{t_1}(\alpha)) \\
%\end{array} 
%\]
%Note that for computing 
%$$g_2^{j}(g_1(\alpha)), \ldots , g_2^{j}(g_1^{t_1}(\alpha))$$
%we use the similar idea which is used in the proof of Lemma \ref{lem:selfcomp}.
%This shows that above computation can be done in $(\log \sqrt{t_2})(\vert G \vert ^{\frac{1}{2}\omega(\frac{4}{3})})$ operations in $F$. By doing similar operations for $g_3, \ldots, g_n$ on all the outputs we can compute the corresponding images to the elements
%of the $n$-cell containing the origin in $\tilde{O}(\vert G \vert ^{\frac{1}{2}\omega(\frac{4}{3})})$
%
%\textbf{step2. computing automorphism evaluations in the $n$-cell containing the origin.}
%
%We want to compute
%$$T_{i_1, \ldots, i_n} = \sum_{\substack{0 \leq j_1 \leq t_1\\ \vdots \\ 0 \leq j_n \leq t_n}} a^{i_1, \ldots , i_n}_{j_1, \ldots , j_n}g_1^{j_1}\cdots g_n^{j_n}(\alpha)$$
%for $0 \leq i_1-1 \leq t_1, \ldots , 0 \leq i_n \leq t_n-1$ which can be done by a rectangular matrix multiplication with cost
%$\tilde{O}(\vert G \vert ^{\frac{1}{2}\omega(\frac{4}{3})})$.
%
%\textbf{step 3. applying $\hat{g}_i$'s.}
%
%We have already computed $\hat{g}_i$ in step 1. Similar to step 1, we can apply $g_i^s$ 
%for $s = 1, 2, 4 , \ldots, t_1$ to $T_{i_1, \ldots , i_n}$ which can be done using $(\log t_i)(\vert G \vert^{\frac{1}{2}\omega
%(\frac{4}{3})})$ operations in $F$. Thus this step can be carried out by $\tilde{O}(\vert G \vert^{\frac{1}{2}\omega
%(\frac{4}{3})})$ operations in $F$. 

\begin{proposition}
Suppose Assumption \ref{assum} holds and $G$ is an abelian group. $l(\osum{K}{G}) \in F[G]$ is computable using $\thecost$ 
operations in $F$.
\end{proposition}

\subsection{Metacyclic Groups}

A Group $G$ is called metacyclic if it has a normal cyclic subgroup, $H$, such that $G/H$ is cyclic. It is known that any group
with a square free order, is metacyclic and elements of a metacyclic group can be presented as 
\begin{equation}\label{eq:metacyclic}
\langle \sigma,\tau: \sigma^n = 1, \tau^{-1}\sigma \tau = \sigma^r, \tau ^m = \sigma^s \rangle
\end{equation}
where $m,n,r,s \in \mathbb{N}, r,s \leq n,$ and $r^m = 1 \mod n , rs = s \mod n$. Moreover we know that all element of a metacyclic
 group can be presented by $$\sigma^i \tau^j, \,\,\, 0\leq i \leq m-1, 0\leq j \leq n-1,$$ 
see \cite[P.88, Proposition 1]{Johnson}, \cite[P.334]{Curtis} for more details. Dihedral group 
$$D_{2n} = \langle \sigma,\tau: \sigma^n =\tau^2 = 1, \sigma \tau = \tau \sigma^{-1} \rangle, $$
is an example of metacyclic groups. another well-known metacyclic group is generalizewd quaternion
 group which can be presented as
 $$Q_n = \langle \sigma,\tau: \sigma^n =\tau^2, \tau \sigma \tau^{-1} = \sigma^{-1} \rangle.$$
 We know that elements of $Q_n$ are of the form 
 $$\sigma^i\tau^j, 0 \leq i \leq 2n-1 , 0\leq j \leq 1.$$
 
Assume $G = \mathrm{Gal(K/F})$ is a group presentable as
$$G = \lbrace \sigma^i \tau^j: 0\leq i \leq n-1, 0 \leq j \leq m-1, m\leq n \rbrace,$$
and $\alpha\in K$. The goal is to compute 
$L(\sigma^i\tau^j (\alpha), \,\, 0\leq i < n, 0 \leq j <m.$
This can be done in three steps.

\textbf{step 1.} apply Lemma \ref{lem:selfcomp} to compute 
$$s_{ij} = \sigma^i\tau^j(\alpha), 0 \leq j < m, 0\leq i < \lceil \sqrt{n}/\sqrt{m} \rceil$$
note that $\lceil \sqrt{n}/\sqrt{m} \rceil m \leq \lceil \sqrt{mn} \rceil$.

\textbf{step 2.} compute $$T_j = L \circ \sigma^{j\sqrt{mn}}, \,\, 0\leq j < \lceil \sqrt{mn}\rceil$$
using Lemma	\ref{lem:transmodcomp}.

%\textbf{step 3.} compute $$\sigma^i (\alpha_j), 0\leq i < n/\sqrt{mn} = \sqrt{m}/\sqrt{n}, 0\leq j <m$$ using Lemma \ref{lem:selfcomp}.

\textbf{step 3.} at this point we want to compute 
$$L(\sigma^i\tau^j(\alpha)) = T_k\cdot(\sigma^i(\alpha_j)).$$
This can be carried out with a rectangular matrix multiplication
$$
\left[ \begin{array}{c}
T_0\\
\hline
T_1\\
\hline
\vdots\\
\hline
T_{\lceil \sqrt{mn} \rceil-1}
\end{array} 
\right]
\cdot
\left[\begin{array}{l}
s_{00} \\
\hline
 \vdots \\
 \hline
s_{0m} \\
 \hline
\vdots \\
\hline
s_{\lceil \sqrt{n}/\sqrt{m} \rceil 0} \\
\hline
\vdots \\
\hline
s_{\lceil \sqrt{n}/\sqrt{m} \rceil m}
\end{array}
\right]^t
$$
which is a $\langle \sqrt{mn},n,\sqrt{mn}\rangle$ multiplication.  
 
% Assume $G$ is given by Equation \ref{eq:metacyclic}, Assumption \ref{assum} holds and 
% $$P(x,y) = \sum_{i= 0}^n \sum_{j=0}^m c_{ij}x^iy^j = \sum_{i= 0}^n x^i\left(\sum_{j=0}^m c_{ij}y^j \right) \in F[x,y]. $$
% We want to get $(P(\sigma,\tau))(\alpha)$. Let $H = \langle x \rangle$ is a normal subgroup of $G$ such that $G/H = \langle yH
%  \rangle$. Since $yxy^{-1}$ belong to $H$, elements in $G$ also be presented as $b^i a^j$. Hence 
% without loss of generality we may assume $n \leq m$. Assume $s = \lceil \sqrt{mn} \rceil$ and
% $$H_i = (\sum_{j=0}^m c_{ij}\tau^j)(\alpha) = \sum_{j=0}^{m/ s} \tau^{js}\left(\sum_{t= 0}^{s}c'_{it}\tau^j(\alpha)\right).$$
% Suppose $T_{ij} = \sum_{t= 0}^{s}c'_{it}\tau^j(\alpha)$. In order to compute each $T_{ij}$ we start by computing 
% $$\tau(\alpha), \tau^2(\alpha), \ldots , \tau^{s}(\alpha).$$
% By Lemma \ref{lem:selfcomp} this can be done using $\tilde{O}((mn)^{3/4 \omega(4/3)}$ operations in $F$. Then to get $T_{it}$, we compute
% $$
%\left(\begin{array}{lll}
%c'_{00} & \cdots & c'_{0s}\\
%\vdots & \vdots & \vdots\\
%c_{m/s0} & \cdots & c_{m/ss}\\
%\vdots & \vdots & \vdots\\
%c_{n0} & \cdots & c_{ns}
%\end{array} \right)
%\cdot
%\left( \begin{array}{l}
%\vv{\tau^0(\alpha)}\\
%\vdots\\
%\vv{\tau^s(\alpha)}
%\end{array}
%\right)
% $$
% where the left matrix is of the size $\sqrt{mn} \times \sqrt{mn}$ and the right one is of the size $\sqrt{mn} \times mn$. Thus the 
% multiplication can be done with cost $O((mn)^{1/2 \omega(2)})$. Note that the total number of $T_{ij}$ is $n\cdot\dfrac{m}{s} = 
% \sqrt{mn}$. So using the similar idea to the abelian case we can apply powers of $\tau^s$ to them, so that we get 
% $T_0, \ldots, T_n$ and the cost is $\tilde{O}((mn)^{3/4\omega(4/3)})$.
% 
% The final step is to compute $$\sigma^0(T_0), \sigma^1(T_1) , \ldots , \sigma^n (T_n).$$
%Note that since $n \leq \sqrt{mn}$, this computation can be done using $\thecost$ operations in $F$, using the same ideas from the abelian case. Now we have proved the following theorem.

\begin{proposition}
Suppose Assumption \ref{assum} holds and $G$ is a metacyclic group. $l(\osum{K}{G}) \in F[G]$ is computable using $\thecost$ 
operations in $F$.
\end{proposition}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "NormalBasisCharZero"
%%% End:
