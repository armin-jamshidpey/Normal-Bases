\section{Computing projections of the orbit sum}
\label{sec:osum}

In this section we present algorithms to compute $s_{\alpha,\ell}$,
when $G$ is either abelian or metacyclic. We start by sketching our
ideas in simplest case, cyclic groups.  We will see that they follow
closely ideas used in \cite{KalSho98} over finite fields.

Suppose $G = \langle g \rangle$, so that given $\alpha$ in $K$ and
$\ell: \K \to \F$, our goal is to compute
\begin{equation}\label{eq:cycproj}
  \ell(g^i(\alpha)), 0 \leq i \leq n-1.
\end{equation}
\citeN{KalSho98} call this the \emph{automorphism projection problem} and
gave an algorithm to solve it in subquadratic time, when $g$ is the
$q$-power Frobenius $\F_{q^n} \to \F_{q^n}$.  The key idea in their
algorithm is to use the baby-steps/giant-steps technique: for a suitable
parameter $t$, the values in \eqref{eq:cycproj} can be rewritten as
$$(\ell \circ g^{tj})(g^i(\alpha)), \quad 0 \leq j < m:=\lceil n/t
\rceil,\quad 0 \leq i <t.$$ First, we compute all $G_i:=g^i(\alpha)$
for $0 \leq i <t$; then all $L_j:=\ell \circ g^{tj}$ for $0 \leq j
<m$, where the $L_j$'s are themselves linear mappings $\K \to \F$.
Finally, a matrix product yields all values $L_j(G_i)$.
 
The original algorithm relies on the properties of the Frobenius
mapping to achieve subquadratic runtime. In our case, we cannot apply
these results directly; instead, we have to revisit the proofs
of~\citeN{KalSho98}, Lemmata 3, 4 and~8, now considering
rectangular matrix multiplication.  Our exponents involve the
constant $\omega(4/3)$, for which we have the upper bound $\omega(4/3)
< 2.654$: this follows from the upper bounds on $\omega(1.3)$ and
$\omega(1.4)$ given in~\cite{LeGall}, and the fact that $k \mapsto
\omega(k)$ is convex~\cite{LoRo83}. In particular, $3/4 \cdot \omega(4/3) <
1.99$. Note also the inequality $\omega(k) \ge 1+k$ for $k\ge 1$,
since $\omega(k)$ describes products with input and output size
$O(n^{1+k})$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Multiple automorphim evaluation and applications}

The key to these algorithms is the remark following
Assumption~\ref{assum}, which reduces automorphism evaluation to
composition of polynomials (over finite fields, this idea goes back
to~\cite{GaSh92}, where it was credited to Kaltofen).

\begin{lemma}\label{lem:modcom}
  Given $\alpha_1,\dots,\alpha_s$ in $\K$ and $g$ in $G =
  \mathrm{Gal}(\K/\F)$, with $s = O(\sqrt{n})$, we can compute
  $g(\alpha_1),\dots,g(\alpha_s)$ in $\tilde
  O(n^{{3}/{4}\omega({4}/{3})})$ operations in $\F$.
\end{lemma}
\begin{proof}
(Compare \cite[Lemma~3]{KalSho98}.) As noted above, for $i\le s$,
  $g(\alpha_i) = \alpha_i(\gamma)$, with $\gamma := g(\bar x) \in \K$.

  Let $t := \lceil n^{3/4} \rceil$, $m:=\lceil n/t\rceil$, and rewrite $\alpha_1 , \ldots , \alpha_s$ as 
$$\alpha_i = \sum_{0 \leq j < m} a_{i,j}\bar x^{tj},$$ where the
  $a_{i,j}$'s are polynomials of degree less than $t$. The next step
  is to compute $\Gamma_i := \gamma^i$, for $i = 0 , \ldots , t$;
  there are $t$ products in $\K$ to do, so this amounts to
  $\tilde{O}(n^{7/4})$ operations in $\F$.

  Having $\Gamma_i$'s in hand, one can form the matrix $\boldsymbol{\Gamma} :=
  \left[ \bar \Gamma_1 ~ \cdots ~\bar \Gamma_t \right]^T$, where each column 
$\bar \Gamma_i$ is
  the coefficient vector of $\Gamma_i$ (with entries in $\F$);
  this matrix is of size $t \times n$. We also form
  $$\mat A := \left[\bar{a}_{1,0} \cdots \bar{a}_{1,m-1} \cdots
    \bar{a}_{s,0} \cdots \bar{a}_{s,m-1}\right]^T,$$ where
  $\bar{a}_{i,j}$ is the coefficient vector of $a_{i,j}$. This matrix 
  has $s m \in O(n^{3/4})$ rows and $n$ columns.

  Compute $\mat B:=\mat A \cdot \boldsymbol{\Gamma}$; as per our
  definition of exponents $\omega(\ .\ )$, this can be done in
  $O(n^{3/4 \omega(4/3)})$ operations in $\F$; the rows of this matrix
  give all $a_{i,j}(\gamma)$.  The last step to get all
  $\alpha_i(\gamma)$ is to write them as $\alpha_i(\gamma) = \sum_{0
    \leq j < m} a_{i,j}(\gamma) \Gamma_t^{j}.$ Using Horner's scheme,
  this takes $O(sm)$ operations in $\K$, which is $\tilde{O}(n^{7/4})$
  operations in $\F$. Since we pointed out that $\omega(3/4) \ge 7/4$,
  the leading exponent in all costs seen so far is
  ${3}/{4}\omega({4}/{3})$.
\end{proof}

\begin{lemma}\label{lem:selfcomp}
Given $\alpha$ in $\K$, $g_1, \ldots , g_{r}$ in $G =
\mathrm{Gal}(\K/\F)$ and positive integers $(s_1, \ldots s_r)$ such
that $\prod_{i = 1}^r s_i = O(\sqrt{n})$, all
  $$g_1^{i_1}\cdots g_r^{i_r}(\alpha) ,\quad \text{~for~} 0 \leq i_j
\leq s_j,\ 1 \leq j \leq r$$ can be computed in $\osumcosttilde$
operations in $\F$.
\end{lemma}
\begin{proof}
(Compare \cite[Lemma~4]{KalSho98}) Suppose $1 \leq m \leq r$ is
  fixed and we have computed 
$$G_{i_1,\dots,i_m}:=g_m^{i_m}\cdots g_1^{i_1}(\alpha)$$ for $0 \leq
  i_j \leq s_j$ if $1 \leq j < m$, and $0 \leq i_m < k_m,$ as well as
  the automorphism $\eta:=g_m^{k_m}$ (by means of its value at $\bar
  x$, as per our convention).
  
 Then, we can get $G_{i_1,\dots,i_m}$ for $0 \leq i_j \leq s_j$ if $1
 \leq j < m$, and $0 \leq i_m < 2k_m$, by computing
 $\eta(G_{i_1,\dots,i_m})$, for all indices $i_1,\dots,i_m$
 available to us, that is, $0 \leq i_j \leq s_j$ if $1 \leq j < m$,
 and $0 \leq i_m < k_m$. This can be carried out using $\osumcost$
 operations in $\F$ by applying Lemma \ref{lem:modcom}.

 Using above doubling method for $g_i$, we have to
 do $O(\log s_i)$ iterations with $\osumcost$ operations in $F$. Hence
 the total cost of this computation is $\osumcosttilde$.

Note that for each $g_i$ we have to compute $g_i^{2^j}$ for $0 \leq j
\leq \log(s_i)$ but this is just a one time computation for each $g_i$
which does not change the total cost. Since $\omega(4/3) \ge 7/4$
\end{proof}

\begin{lemma}\label{lem:transmodcomp}
Under Assumption \ref{assum}, for $\lbrace s_1, \ldots, s_r \rbrace \subset \mathbb{N}$
such that $\prod_{i = 1}^r s_i = O(\sqrt{n})$
 and $L: K\rightarrow F$ is a linear projection. The linear maps
$$L \circ g_1^{i_1} \cdots g_r^{i_r}, \,\, 1\leq j \leq r, 0 \leq i_j < s_j, $$ can be computed using $\osumcosttilde$ operations in $F$. 
\end{lemma} 

\begin{proof}
Let $T_{i_1\cdots i_r} = L \circ g_1^{i_1}\cdots g_r^{i_r}$. Here we use a doubling method. Assume $1 \leq m \leq r$ is fixed and we have have access to 
$$T_{i_1\cdots i_m}, \, \mathrm{for}\,\, 1\leq j < m, 0 \leq i_j \leq s_j, 0 \leq i_m \leq k_m.$$
Then we can compute 
$$T_{i_1\cdots i_m}, \, , 1 \leq j < m, 0 \leq i_j \leq s_j, 0 \leq i_m \leq 2k_m,$$
in the following way. We compute 
$T_{i_1 \cdots i_m} \circ g_m^{k_m}, \,\, 1\leq j \leq m, 0 \leq i_j \leq s_j.$
 Since a linear projection is determined by its action on a basis, such as 
$\lbrace\bar{x}^i: 0\leq i \leq n-1 \rbrace$, the corresponding row vector to $T_{i_1 \cdots i_m} \circ g_m^{k_m}$ is 
\begin{equation}\label{eq:projvec}
\begin{bmatrix} T_{i_1 \cdots i_m}(u_{m,k_m}^0) & \cdots & T_{i_1 \cdots i_m}(u_{m,k_m}^{n-1}) \end{bmatrix}
\end{equation}
where $u_{m,k_m} = g_m^{k_m}(\bar{x})$. In order to compute the above row vector for a fixed index $i_1 \cdots i_m$, let $s = \lceil n^{1/4} \rceil$, compute
$$\overline{T_{i_1 \cdots i_m}} = \left[\begin{array}{c|c|c}
u_{m,k_m}^0\cdot T_{i_1 \cdots i_m} & \cdots & u_{m,k_m}^{s-1}\cdot T_{i_1 \cdots i_m}
\end{array} \right]$$
and multiply it by 
$$ U = \left[
\begin{array}{c}
u_{m,k_m}^0\\
\hline
u_{m,k_m}^{s}\\
\hline
u_{m,k_m}^{2s}\\
\hline
\vdots\\
\hline
u_{m,k_m}^{(\lceil n^{3/4} \rceil-1)s}
\end{array} \right].$$
Note that the term $u_{m,k_m}^i \cdot T_{i_1 \cdots i_m}$, is the so called transposed modular multiplication and one can apply the algorithm introduced in \cite{Shoup} to compute it. It is clear that $U$ is of size 
$\lceil n^{3/4} \rceil \times n$ and $\overline{T_{i_1 \cdots i_m}}$ is an $n \times n^{1/4}$. Finally the vectors in \eqref{eq:projvec}
is given by
$$
U \cdot \left[\begin{array}{c|c|c|c|c}
\overline{T_{0 \cdots 0}} & \cdots &\overline{T_{i_1 \cdots i_{m-1}0}} & \cdots & \overline{T_{k_1 \cdots k_m}}
\end{array}\right].
$$
This completes the doubling method.

Note that in the last step of above doubling method for a fixed index $t$ we have $k_t = s_t/2$. Hence for $g_t$ the above computation can be done 
using $O(\log s_t)$ times the cost of the matrix multiplication which is $\osumcost$. Since we should repeat the above process for each $g_t$ the total cost is
$\osumcosttilde$ 
\end{proof}

%% At this point we have enough tools to see how the computation is done in the cyclic case. Moreover, we can use the above lemmas
%% to give an algorithm for a more general case, namely the abelian case. With a little bit more work we can state an algorithm 
%% which solves the automorphism projection problem when $G = \lbrace a^ib^j \rbrace$ where $m \leq n$. As an specific case this
%% solves the problem for metacyclic groups.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Abelian Groups}\label{ssec:proj_abelian}

Assume $G$ is an abelian group presented as 
$$ \langle g_1, \ldots , g_r: g_{1}^{e_1} = \cdots = g_{r}^{e_r} = 1 \rangle$$
 where $ e_i \in \mathbb{N}$
is the order of $g_i$ and $n = e_1 \cdots e_r$. Moreover, let $s_i = \lceil	\sqrt{e_i \rceil}$ for $ 1\leq i \leq r$.
 Our goal is to compute 
\begin{equation}\label{eq:abelian}
L (g_1^{i_1},  \ldots, g_r^{i_r}(\alpha)), \, 1 \leq j \leq r, 0 \leq i_j \leq e_j
\end{equation}
 where $L = \begin{bmatrix} l_1 & \cdots l_n \end{bmatrix}$ is a given projection from $K$ to $F$. 
similar to the cyclic case, the elements in \eqref{eq:abelian} can be presented as 
\begin{equation}
\begin{split}
(L \circ g_1^{s_1j_1} \cdots g_r^{s_rj_s})\cdot (g_1^{i_1} \cdots g_r^{i_r}(\alpha))\\ 1\leq m \leq r, 0\leq i_m < s_m, 0 \leq j_m < s_m
\end{split}
\end{equation}
note that $T_{j_1\cdots j_r} = L \circ g_1^{s_1j_1} \cdots g_r^{s_rj_s}$ are linear projections 
presented as row vectors and $g_1^{i_1} \cdots g_r^{i_r}(\alpha)$ are field elements presented as column vectors. All elements in \eqref{eq:abelian} can be computed in 3 steps.

\textbf{Step 1.} Apply Lemma \ref{lem:selfcomp} to get 
$$g_1^{i_1} \cdots g_r^{i_r}(\alpha), \,\, 1\leq m \leq r, 0\leq i_m < s_m,$$
with cost $\osumcosttilde$

\textbf{Step 2.} Use Lemma \ref{lem:transmodcomp} to compute 
$$T_{j_1\cdots j_r}, \,\, 1\leq m \leq r, 0 \leq j_m < s_m,$$
with cost $\osumcosttilde$

\textbf{Step 3.} The last step is to the following matrix multiplication

$$
\left[ \begin{array}{c}
T_{00\cdots 0}\\
\hline
T_{10\cdots 0}\\
\hline
\vdots\\
\hline
T_{s_1 \cdots s_r}
\end{array} \right]
\cdot
\left[\begin{array}{c}
g_1^{0}\cdots g_r^{0}(\alpha) \\
\hline
g_1^{1}\cdots g_r^{0}(\alpha) \\
\hline
\cdots \\
\hline
g_1^{s_1}\cdots g_r^{s_r}(\alpha)  
\end{array}\right]^t.
$$
Using results for rectangular matrix multiplication we can compute the above product in $O(n^{1/2\omega(2)})$ operations in $F$.
The above computation can be done in $\osumcosttilde$ operations in $F$ which produces a $\lceil \sqrt{n} \rceil \times \lceil
 \sqrt{n} \rceil$ matrix which contains all the projections. Concatenating the rows of the mentioned matrix forms a vector in
 $F[G]$ which is the corresponding vector to $\osum{G}{F}$ with respect to the basis $G$. This means we have proved the following
 proposition.

\begin{proposition}
Suppose Assumption \ref{assum} holds and $G$ is an abelian group. $l(\osum{K}{G}) \in F[G]$ is computable using $\thecost$ 
operations in $F$.
\end{proposition}

\subsection{Metacyclic Groups}

A Group $G$ is called metacyclic if it has a normal cyclic subgroup, $H$, such that $G/H$ is cyclic. It is known that any group
with a square free order, is metacyclic and elements of a metacyclic group can be presented as 
\begin{equation}\label{eq:metacyclic}
\langle \sigma,\tau: \sigma^n = 1, \tau^{-1}\sigma \tau = \sigma^r, \tau ^m = \sigma^s \rangle
\end{equation}
where $m,n,r,s \in \mathbb{N}, r,s \leq n,$ and $r^m = 1 \mod n , rs = s \mod n$. Moreover we know that all element of a metacyclic
 group can be presented by $$\sigma^i \tau^j, \,\,\, 0\leq i \leq m-1, 0\leq j \leq n-1.$$ 
for more details on metacyclic groups see \cite[P.88, Proposition 1]{Johnson}, \cite[P.334]{Curtis}. for constructing some 
Metacyclic extensions see \cite{Kida}.

. Dihedral group 
$$D_{2n} = \langle \sigma,\tau: \sigma^n =\tau^2 = 1, \sigma \tau = \tau \sigma^{-1} \rangle, $$
is an example of metacyclic groups. another well-known metacyclic group is generalized quaternion
 group which can be presented as
 $$Q_n = \langle \sigma,\tau: \sigma^n =\tau^2, \tau \sigma \tau^{-1} = \sigma^{-1} \rangle.$$
 We know that elements of $Q_n$ are of the form 
 $$\sigma^i\tau^j, 0 \leq i \leq 2n-1 , 0\leq j \leq 1.$$
 
Assume $G = \mathrm{Gal(K/F})$ is a group presentable as
$$G = \lbrace \sigma^i \tau^j: 0\leq i < n, 0 \leq j < m, m\leq n \rbrace,$$
and $\alpha\in K$. The goal is to compute 
$L(\sigma^i\tau^j (\alpha), \,\, 0\leq i < n, 0 \leq j <m.$
This can be done in three steps.

\textbf{step 1.} apply Lemma \ref{lem:selfcomp} to compute 
$$s_{ij} = \sigma^i\tau^j(\alpha), 0 \leq j < m, 0\leq i < \lceil \sqrt{n}/\sqrt{m} \rceil$$
note that $\lceil \sqrt{n}/\sqrt{m} \rceil m \leq \lceil \sqrt{mn} \rceil$.

\textbf{step 2.} compute $$T_j = L \circ \sigma^{j\sqrt{mn}}, \,\, 0\leq j < \lceil \sqrt{mn}\rceil$$
using Lemma	\ref{lem:transmodcomp}.

\textbf{step 3.} at this point we want to compute 
$$L(\sigma^i\tau^j(\alpha)) = T_k\cdot(\sigma^i(\alpha_j)).$$
This can be carried out with a rectangular matrix multiplication
$$
\left[ \begin{array}{c}
T_0\\
\hline
T_1\\
\hline
\vdots\\
\hline
T_{\lceil \sqrt{mn} \rceil-1}
\end{array} 
\right]
\cdot
\left[\begin{array}{l}
s_{00} \\
\hline
 \vdots \\
 \hline
s_{0m} \\
 \hline
\vdots \\
\hline
s_{\lceil \sqrt{n}/\sqrt{m} \rceil 0} \\
\hline
\vdots \\
\hline
s_{\lceil \sqrt{n}/\sqrt{m} \rceil m}
\end{array}
\right]^t
$$
which is a $\langle \sqrt{mn},n,\sqrt{mn}\rangle$ multiplication. 

We note the above algorithm works for a class of groups which includes metacyclic case. Since if $G$ is metacyclic, 
$H = \langle \sigma \rangle \unlhd G$ and $G/H = \langle \tau H \rangle$, then both $\tau \sigma \tau^{-1}$ and 
$\tau^{-1} \sigma \tau$ belong to $H$. This implies elements of $G$ can be presented either as $\sigma^i\tau^j$
or $\tau^j\sigma^i$. Thus without loss of generality we can assume the $\textrm{Ord}(\tau) \leq \textrm{Ord}(\sigma)$.

Similar to the abelian case the final output of the above algorithm is a $\lceil \sqrt{n} \rceil \times \lceil \sqrt{n} \rceil $
matrix and $l(\osum{G}{K})$ can be calculated in the same way. Hence we have proved the following proposition.
 
\begin{proposition}
Suppose Assumption \ref{assum} holds and $G$ is a metacyclic group. $l(\osum{K}{G}) \in F[G]$ is computable using $\thecost$ 
operations in $F$.
\end{proposition}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "NormalBasisCharZero"
%%% End:
