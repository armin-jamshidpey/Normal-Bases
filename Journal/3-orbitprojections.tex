\section{Computing projections of the orbit sum}
\label{sec:osum}

In this section we present an algorithm to compute $s_{\alpha,\ell}$
when $G=\{g_1,\dots,g_n\}$ is {\em polycyclic} (we give a definition
of this family of groups and recall some well known results about them
in Subsection~\ref{ssec:proj_abelian}). To motivate our algorithm,
we start by the simple case of a {\em cyclic} group.  We will see that
they follow closely ideas used by \cite{KalSho98} over finite fields.

Suppose $G = \langle g \rangle$, so that given $\alpha$ in $\K$ and
$\ell: \K \to \F$, our goal is to compute
\begin{equation}
  \label{eq:cycproj}
  \ell(g^i(\alpha)), ~~\mbox{for}~ 0\leq i\leq n-1.
\end{equation}
\cite{KalSho98} call this the \emph{automorphism projection problem} and
gave an algorithm to solve it in subquadratic time, when $g$ is the
$q$-power Frobenius $\mathbb{F}_{q^n} \to \mathbb{F}_{q^n}$.  The key idea in their
algorithm is to use the baby-steps/giant-steps technique: for a suitable
parameter $t$, the values in \eqref{eq:cycproj} can be rewritten as
\[
  (\ell \circ g^{tj})(g^i(\alpha)), ~~\mbox{for}~ 0 \leq j < m:=\lceil n/t
  \rceil ~\mbox{and}~ 0 \leq i <t.
\]
First, we compute all $G_i:=g^i(\alpha)$ for $0 \leq i <t$.  Then we compute
all $L_j:=\ell \circ g^{tj}$ for $0 \leq j <m$, where the $L_j$'s are
themselves linear mappings $\K \to \F$.  Finally, a matrix product yields
all values $L_j(G_i)$.

The original algorithm of \cite{KalSho98} relies on the properties of
the Frobenius mapping to achieve subquadratic runtime. In our case, we
cannot apply these results directly; instead, we have to revisit the
proofs of~\cite[Lemmata 3 and 4]{KalSho98}, now considering
rectangular matrix multiplication.  Our exponents involve the constant
$\omega(4/3)$, for which we have the upper bound $\omega(4/3) <
2.654$: this follows from the upper bounds on $\omega(1.3)$ and
$\omega(1.4)$ given by~\cite{LeGall}, and the fact that $k \mapsto
\omega(k)$ is convex~\citep{LoRo83}. In particular, $3/4 \cdot
\omega(4/3) < 1.99$. Note also the inequality $\omega(k) \ge 1+k$ for
$k\ge 1$, since $\omega(k)$ describes products with input and output
size $O(n^{1+k})$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Multiple automorphism evaluation and applications}

The key to the algorithms below is the remark following
Assumption~\ref{assum}, which reduces automorphism evaluation to
modular composition of polynomials.  Over finite fields, this idea
goes back to~\cite{GaSh92}, where it is credited to Kaltofen.

For instance, given $g \in G$ (by means of $\gamma:=g(\xbar)$), we can
deduce $g^2 \in G$ (again, by means of its image at $\xbar$) as
$\gamma(\gamma)$; this can be done with $\tilde{O}(n^{(\omega+1)/2})$
operations in $\F$ using Brent and Kung's modular composition
algorithm~\citep{BrKu78}. The algorithms below describe similar
operations along these lines, involving several simultaneous
evaluations. In this subsection, we work under Assumption~\ref{assum}
and we make no special assumption on $G$.

\begin{lemma}
  \label{lem:modcom}
  Given $\alpha_1,\dots,\alpha_s$ in $\K$ and $g$ in $G =
  \mathrm{Gal}(\K/\F)$, with $s = O(\sqrt{n})$, we can compute
  $g(\alpha_1),\dots,g(\alpha_s)$ with $\tilde
  O(n^{(3/4)\cdot\omega(4/3)})$ operations in $\F$.
\end{lemma}
\begin{proof}
(Compare \citealt[Lemma~3]{KalSho98}) As noted above, for $i\le s$,
  $g(\alpha_i) = \alpha_i(\gamma)$, with $\gamma := g(\xbar) \in \K$.
  Let $t := \lceil n^{3/4} \rceil$, $m:=\lceil n/t\rceil$, and rewrite $\alpha_1 , \ldots , \alpha_s$ as 
$$\alpha_i = \sum_{0 \leq j < m} a_{i,j}\xbar^{tj},$$ where the
  $a_{i,j}$'s are polynomials of degree less than $t$. The next step
  is to compute $\gamma_i := \gamma^i$, for $i = 0 , \ldots , t$.
  There are $t$ products in $\K$ to perform, so this amounts to
  $\tilde{O}(n^{7/4})$ operations in $\F$.

  Having $\gamma_i$'s in hand, one can form the matrix
  $\boldsymbol{\Gamma} := \left[ \Gamma_0 ~ \cdots ~ \Gamma_{t-1}
    \right]^T$, where each column $\Gamma_i$ is the coefficient vector
  of $\gamma_i$ (with entries in $\F$); this matrix has $t \in
  O(n^{3/4})$ rows and $n$ columns. We also form
  $$\mat A := \left[{A}_{1,0} \cdots {A}_{1,m-1} \cdots
    {A}_{s,0} \cdots {A}_{s,m-1}\right]^T,$$ where
  ${A}_{i,j}$ is the coefficient vector of $a_{i,j}$. This matrix 
  has $s m \in O(n^{3/4})$ rows and $t \in O(n^{3/4})$ columns.

  Compute $\mat B:=\mat A\, \boldsymbol{\Gamma}$; as per our
  definition of exponents $\omega(\cdot )$, this can be done in
  $O(n^{(3/4)\cdot \omega(4/3)})$ operations in $\F$, and the rows of this matrix
  give all $a_{i,j}(\gamma)$.  The last step to get all
  $\alpha_i(\gamma)$ is to write them as $\alpha_i(\gamma) = \sum_{0
    \leq j < m} a_{i,j}(\gamma) \gamma_t^{j}.$ Using Horner's scheme,
  this takes $O(sm)$ operations in $\K$, which is $\tilde{O}(n^{7/4})$
  operations in $\F$. Since $(3/4)\cdot\omega(4/3) \ge 7/4$,
  the leading exponent in all costs seen so far is
  $(3/4)\cdot\omega(4/3)$.
\end{proof}


\begin{lemma}\label{lem:selfcomp}
  Consider $g_1, \ldots g_r$ in $G=\mathrm{Gal}(\K/\F)$, positive
  integers $(s_1, \ldots s_r)$ and elements $\alpha_{i_1, \dots, i_r}$
  in $\K$, for $i_m=0,\dots,s_m$, $m=1,\dots,r$. If $\prod_{i = 1}^r
  s_i = O(\sqrt{n})$ and $r = O(\log(n))$, we can compute
  $$g_r^{i_r}\cdots g_1^{i_1}(\alpha_{i_1, \dots, i_r})
  \text{~for~} i_m=0,\dots,s_m,\ m=1,\dots,r
  $$  using $\thecost$ operations in $\F$.
\end{lemma}
\begin{proof}
  Define $\mathcal I = \{(i_1,\dots,i_r) \mid i_m=0,\dots,s_m
  \text{~for~} m=1,\dots,r\}$. For $(i_1,\dots,i_r)$ in $\mathcal I$
  and non-negative integers $\ell_1,\dots,\ell_r$, define
  $$\alpha_{i_1, \dots, i_r}^{(\ell_1,\dots,\ell_r)} 
  =g_r^{\ell_r} \cdots g_1^{\ell_1}(\alpha_{i_1, \dots, i_r}).$$  
  Assume then that
  for some $t$ in $\{0,\dots,r-1\}$, we know 
  $$S_t=(\alpha_{i_1, \dots,i_r}^{(i_1, \dots, i_{t},0, \dots, 0)} \ \mid \ (i_1,\dots,i_r)\in \mathcal I);$$ we show how to compute 
  $$S_{t+1}=(\alpha_{i_1, \dots,i_r}^{(i_1, \dots, i_{t+1},0, \dots,
    0)} \ \mid \ (i_1,\dots,i_r)\in \mathcal I).$$ Since our input is
  $S_0$, it will be enough to go through this process for all values
  of $t$ to obtain the output $S_r$ of the algorithm.
  
  For a given index $t$, and for $m \ge 0$
  define further 
  $$S_{t,m}=(\alpha_{i_1, \dots,i_r}^{(i_1, \dots, i_{t},i_{t+1} \bmod
    2^m, 0,\dots, 0)} \ \mid \ (i_1,\dots,i_r)\in \mathcal I);$$ in
  particular, $S_{t,0} = S_t$ and $S_{t,\lfloor
    \log_2(s_{t+1})\rfloor+1} = S_{t+1}$.  Hence, given $S_{t,m}$, it
  is enough to show how to compute $S_{t,m+1}$, for indices
  $m=0,\dots,\lfloor \log_2(s_{t+1})\rfloor$.  This is done by writing
  $$S_{t,m+1}=
  (\beta_{i_1, \dots,i_r,t,m} \ \mid \ (i_1,\dots,i_r)\in \mathcal I),$$
  with
  $$\beta_{i_1, \dots,i_r,t,m}
  =\begin{cases}
  \alpha_{i_1, \dots,i_r}^{(i_1, \dots, i_{t},i_{t+1} \bmod 2^m,0, \dots, 0)}  \text{~if~} 
  i_{t+1} \bmod 2^{m+1} = i_{t+1} \bmod 2^m \\[2mm]
  g_{t+1}^{2^{m}}(\alpha_{i_1, \dots,i_r}^{(i_1, \dots, i_{t},i_{t+1} \bmod 2^m,0, \dots, 0)}) 
  \text{~otherwise.}
  \end{cases}$$
  The automorphisms $g_{t+1}^{2^m}$ can be computed iteratively by modular
  composition; the bottleneck is the application of  $g_{t+1}^{2^m}$
  to a subset of $S_{t,m}$. Using Lemma \ref{lem:modcom}, since 
  $S_{t,m}$ has $O(\sqrt n)$ elements, this takes $\thecost$ 
  operations in $\F$.
  
  For a given index $t$, this is repeated $\lfloor
  \log_2(s_{t+1})\rfloor \le \log_2(s_{t+1}) + 1$ times. Adding up for
  all indices $t$, this amounts to $O(\log (s_1 \cdots s_r)+r)$
  repetitions, which is $O(\log(n))$ by assumption; the conclusion
  follows.
\end{proof}

We now present dual versions of the previous two lemmas (note that
\cite{KalSho98} also have such a discussion). Seen as an $\F$-linear
map, the operator $g:\alpha \mapsto g(\alpha)$ admits a transpose,
which maps an $\F$-linear form $\ell:\K\to\F$ to the $\F$-linear form
$\ell \circ g: \alpha \mapsto \ell(g(\alpha))$.  The {\em
  transposition principle}~\citep{KaKiBs88,CaKaYa89} implies that if a
linear map $\F^N \to \F^M$ can be computed in time $T$, its transpose
can be computed in time $T+O(N+M)$. In particular, given $s$ linear
forms $\ell_1,\dots,\ell_s$ and $g$ in $G$, transposing
Lemma~\ref{lem:modcom} shows that we can compute $\ell_1 \circ
g,\dots,\ell_s \circ g$ in time $\osumcosttilde$. The following lemma
sketches the construction.

\begin{lemma}
  \label{lem:modcomT}
  Given $\F$-linear forms $\ell_1,\dots,\ell_s:\K\to \F$ and $g$ in $G =
  \mathrm{Gal}(\K/\F)$, with $s = O(\sqrt{n})$, we can compute
  $\ell_1\circ g,\dots,\ell_s \circ g$ 
 using $\thecost$ operations in $\F$.
\end{lemma}
\begin{proof}
  Given $\ell_i$ by its values on the power basis $1,\xbar,\dots,\xbar^{n-1}$, $\ell_i \circ g$ is represented by its values at
  $1,\gamma,\dots,\gamma^{n-1}$, with $\gamma := g(\xbar)$. 

  Let $t,m$ and $\gamma_0,\dots,\gamma_t$ be as in the proof of
  Lemma~\ref{lem:modcom}. Compute the ``giant steps''
  $\gamma_t^j = \gamma^{tj}$, $j=0,\dots,m-1$ and for $i=1,\dots,s$
  and $j=0,\dots,m-1$, deduce the linear forms $L_{i,j}$ defined by
  $L_{i,j}(\alpha) := \ell_i(\gamma^{tj}\alpha)$ for all $\alpha$ in
  $\K$. Each of them can be obtained by a {\em transposed
    multiplication} in time $\tilde{O}(n)$~\citep[Section~4.1]{Shoup},
  so that the total cost thus far is $\tilde{O}(n^{7/4})$.

  Finally, multiply the $(sm \times n)$ matrix with entries the
  coefficients of all $L_{i,j}$ (as rows) by the $(n \times t)$ matrix with
  entries the coefficients of $\gamma_0,\dots,\gamma_{t-1}$ (as columns) to
  obtain all values $\ell_i(\gamma^j)$, for $i=1,\dots,s$ an
  $j=0,\dots,n-1$.  This can be accomplished with
  $O(n^{(3/4)\cdot\omega(4/3)})$ operations in~$\F$.
\end{proof}

From this, we deduce the transposed version of Lemma~\ref{lem:selfcomp},
whose proof follows the same pattern.

\begin{lemma}
  \label{lem:transmodcomp}
  Consider $g_1, \ldots, g_r$ in $G=\mathrm{Gal}(\K/\F)$, positive
  integers $(s_1, \ldots, s_r)$ and $\F$-linear forms $\ell_{i_1,
    \dots, i_r}$, for $i_m=0,\dots,s_m$, $m=1,\dots,r$. If $\prod_{i =
    1}^r s_i = O(\sqrt{n})$ and $r = O(\log(n))$, we can compute
  $$\ell_{i_1, \dots, i_r} \circ g_r^{i_r}\cdots g_1^{i_1}
  \text{~for~} i_m=0,\dots,s_m,\ m=1,\dots,r
  $$  using $\thecost$ operations in $\F$.
\end{lemma} 
\begin{proof}
  We proceed as in Lemma~\ref{lem:selfcomp}, reversing the order of
  the steps. Using the same index set $\mathcal I$ as before, define,
  for $(i_1,\dots,i_r)$ in $\mathcal I$ and non-negative integers
  $k_1,\dots,k_r$
  $$\ell_{i_1,\dots,i_r}^{(k_1,\dots,k_r)} =\ell_{i_1,\dots,i_r} \circ g_r^{k_r}\cdots g_1^{k_1}.$$
  For $t=r,\dots,0$, assuming that
  we know 
  $$L_{t+1} = (\ell_{i_1, \dots,i_r}^{(0, \dots, 0,i_{t+1},\dots,i_r)} \ \mid
  \ (i_1,\dots,i_r)\in \mathcal I),$$ we compute 
  $$L_{t}=(\ell_{i_1, \dots,i_r}^{(0, \dots, 0,i_{t},i_{t+1},\dots,i_r)}
  \ \mid \ (i_1,\dots,i_r)\in \mathcal I).$$
  This time, for $m \ge 0$, we set
  $$L_{t+1,m} = (\ell_{i_1, \dots,i_r}^{(0, \dots, 0,\lfloor i_{t}
    \rfloor_m,i_{t+1},\dots,i_r)} \ \mid \ (i_1,\dots,i_r)\in \mathcal
  I),$$ where for a non-negative integer $x$, $\lfloor x \rfloor_m = x
  - (x \bmod (2^m-1))$ is obtained by setting to zero the coefficients
  of $1,2,\dots,2^{m-1}$ in the base-two expansion of $x$.

  Starting from $L_{t+1} = L_{t, \lceil \log_2(s_t) \rceil +1}$, we
  compute all $L_{t+1,m}$ for $m= \lceil \log_2(s_t) \rceil,\dots,0$,
  since $L_{t+1,0} = L_{t}$. This is done essentially as in
  Lemma~\ref{lem:selfcomp}, but using Lemma~\ref{lem:modcomT} this
  time, in order to do right-composition by $g_t^{2^m}$.
  The cost analysis is as in Lemma~\ref{lem:selfcomp}.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Computing the orbit sum projection for polycyclic groups}
\label{ssec:proj_abelian}

Our main algorithm in this section applies to a family of groups known
as {\em polycyclic}; see~\cite[Chapter 8]{HoEiOb05} for more details
on such groups.

Our group $G$ is called polycyclic if it has a normal series
$$G = G_{r} \unrhd G_{r-1} \unrhd \cdots \unrhd G_1 \unrhd G_{0} =
1,$$ where $G_{j}/G_{j-1}$ is cyclic; without loss of generality, we
assume that $G_{j-1 } \ne G_{j}$ holds for all $j$, so that $r$ is
$O(\log(n))$, with $n=|G|$. Finitely generated nilpotent or abelian
groups are polycyclic. In general any finite solvable group is
polycyclic; our key families of examples in the next section (abelian
and metacyclic groups) thus fit into this category.

If $G$ is polycyclic then, up to renumbering, its 
elements can be written as
\[\label{eq:polycyclicgrp}
g_r^{i_r} \cdots g_1^{i_1}, \text{~with~} 0 \leq i_j < e_j
\text{~for~} 1 \leq j \leq r,\
\]
where $G_{j}/G_{j-1} = \langle g_{j}G_{j-1} \rangle$ and $e_j=\vert
G_{j}/G_{j-1}\vert$. Elements of $\K[G]$, or $\F[G]$ are written as
polynomials $\sum_{i_1,\dots,i_r} c_{i_1,\dots,i_r} {g_r}^{i_r} \cdots
{g_1}^{i_1}$, with $0\le i_j < e_j$ for all~$j$, and coefficients 
in either $\K$ or $\F$.


%% Assume elements of a polycyclic group $G$ are presented as in
%% \eqref{eq:polycyclicgrp}, so that elements of $\F[G]$ are written as
%% polynomials $$\sum_{i_1,\dots,i_r} c_{i_1,\dots,i_r} {g_r}^{i_r}
%% \cdots {g_1}^{e_1},$$ with $0\le i_j < e_j$ for all~$j$.

\begin{proposition}\label{prop:polycyclic}
  Suppose that $G$ is polycyclic, with notation as above. For $\alpha$ in
  $\K$ and $\ell:\K\to\F$, $s_{\alpha,\ell} \in \F[G]$, as defined
  in~\eqref{def:s_alpha_ell}, can be computed using $\osumcosttilde$
  operations in $\F$.
\end{proposition}
\begin{proof}
Our goal is to compute
\begin{equation} \label{eq:polycyclic}
  \ell (g_r^{i_r}  \ldots g_1^{i_1}(\alpha)),
\end{equation}
for all indices such that $0 \leq i_j < e_j$ holds for $1 \leq j \leq
r$; here, $\ell$ is an $\F$-linear projection $\K\to \F$.

Our construction is inspired by that sketched in the cyclic case.
Define $z$ to be the unique index in $\{1,\dots,r\}$ such that
$e_1\cdots e_{z-1} < \sqrt{n}$ and $e_1 \cdots e_{z} \geq
\sqrt{n}.$ Then, all elements in \eqref{eq:polycyclic} can be computed
with the following steps, the sum of whose costs proves the
proposition.

\smallskip\noindent \textbf{Step 1.} Apply Lemma \ref{lem:selfcomp},
with $\alpha_{i_1,\dots,i_r} = \alpha$ for all $i_1,\dots,i_r$, to get
$$ G_{i_z,\dots,i_1}=g_z^{i_z} \cdots
g_1^{i_1}(\alpha),$$ for all indices $i_1,\dots,i_z$ such that $0\leq
i_m < e_m$ holds for $m=1,\dots,z-1$ and $0\leq i_z <
\lceil{\sqrt{n}}/(e_1 \cdots e_{z-1})\rceil$.  This amounts to taking
$s_1=e_1,\dots,s_{z-1}=e_{z-1}$, $s_z=\lceil {\sqrt{n}}/(e_1 \cdots
e_{z-1})\rceil$ and $s_m = 1$ for $m > z$ in the lemma.  For the lemma to
apply, we have to check that the product of these indices
$s_1,\dots,s_r$ is $O(\sqrt n)$. 
Indeed, this product is at most 
\begin{align*}
e_1\cdots e_{z-1} \left ( \frac{\sqrt n}{e_1\cdots e_{z-1}} + 1\right )
& \le \sqrt n + e_1\cdots e_{z-1} \le 2 \sqrt n.
\end{align*}
Hence, the lemma applies, and the cost of this step is $\osumcosttilde$.

\smallskip\noindent\textbf{Step 2.} Compute $G_z =g_z^{s_z}$, for $s_z$
as above. The cost is that of $O(\log(n))$ modular compositions, which
is negligible compared to the cost of the previous step.

\smallskip\noindent\textbf{Step 3.} Use Lemma \ref{lem:transmodcomp}
with $\ell_{i_r,\dots,i_1} = \ell$ for all $i_1,\dots,i_r$, to compute 
\begin{align*}
L_{j_r,\dots,j_z} &= \ell \circ (g_{r}^{j_{r}}\cdots g_{z+1}^{j_{z+1}}G_z^{j_z})\\ 
&= \ell \circ (g_{r}^{j_{r}}\cdots g_{z+1}^{j_{z+1}}g_{z}^{s_z j_z}),
\end{align*}
for all indices $0 \leq j_{z} < \lceil {e_{z}}/{s_z}\rceil$ and $0
\leq j_m < e_m$ for $m > z$. This amounts to using the lemma with
indices $s'_1=\cdots=s'_{z-1}=1$, $s'_z = \lceil {e_{z}}/{s_z}\rceil$
and $s'_m = e_m$ for $m > z$. Again, we have to verify that $s'_1
\cdots s'_r$ is $O(\sqrt n)$.  Indeed, we have
\begin{align*}
s'_1 \cdots s'_r  = \left \lceil \frac{e_{z}}{s_z}\right \rceil e_{z+1} \cdots e_r
&\le \left (\frac{e_{z}}{s_z} +1 \right) e_{z+1} \cdots e_r\\
&\le \frac{e_{z} \cdots e_r}{s_z} + e_{z+1} \cdots e_r.
\end{align*}
By definition, we have $s_z \ge \sqrt{n}/(e_1\cdots e_{z-1})$, so
$e_z \cdots e_r/s_z \le e_1 \cdots e_r /\sqrt{n} =\sqrt{n}$.
Because we assume $e_1 \cdots e_{z} \geq \sqrt{n}$, the second term is
also at most $\sqrt{n}$, so the product $s'_1 \cdots s'_r$ is at most
$2 \sqrt{n}$. Hence, Lemma \ref{lem:transmodcomp} applies, and computes
all $L_{j_r,\dots,j_z}$ using $\osumcosttilde$ operations
in $\F$.

\smallskip\noindent\textbf{Step 4.} Multiply the matrix with rows the
coefficients of all $L_{j_r,\dots,j_z}$ by the matrix whose columns
are the coefficients of all $G_{i_z,\dots,i_1}$. This yields the
values
$$\ell( 
g_{r}^{j_{r}}\cdots g_{z+1}^{j_{z+1}}g_{z}^{s_z j_z+i_z} g_{z-1}^{i_{z-1}}\cdots
g_1^{i_1}(\alpha)),$$
for indices as follows:
\begin{itemize}
\item[$\bullet$] $0 \le i_m < e_m$ for $m=0,\dots,z-1$;
\item[$\bullet$] $0 \le i_z < s_z$ and $0 \le j_z< \lceil  e_z/s_z\rceil$;
\item[$\bullet$] $0 \le j_m < e_m$ for $m=z+1,\dots,r$.
\end{itemize}
This shows that we obtain all required values. We compute this product
in $O(n^{(1/2)\cdot\omega(2)})$ operations in $\F$, which is in
$\osumcost$.
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "NormalBasisCharZero"
%%% End:
